\chapter{Introduction}

A \emph{centralized system} is one in which the core functionality of the system
is dictated by a single body that single-handedly determines the use and
behavior of the system. Any privacy guarantees are at the sole discretion of the
system. Common examples of centralized systems of relevance to this dissertation
are Twitter, Signal, and centralized censorship systems. In each of these cases
the centralized systems have multiple servers spread across their areas of
operation, around the world for global services such as Twitter and Signal and
spread throughout a censored region in the case of censorship systems. However
these servers function as a whole, often running the exact same code, and
sharing the data they collect with the centralized system of use.

Users of centralized systems have no inherent guarantee of \emph{privacy} for
their data. How their data is collected and used is at the sole discretion of
the service. Services often offer vague claims as to their privacy guarantees
for users: social media sites collect a mass amount of information on its users,
extending past the content provided to the site; Facebook collects information
on which people users interact with, including who you communicate with and
which content you interact most often with, similarly Twitter collects metadata
information on its users including browser information and operating
system~\cite{fb-privacy,twitter-privacy}.

With the current state of the internet functioning primarily under centralized
systems that function on the data that users generate or inherently provide
users might question ``Can privacy exist in centralized systems?''. This
dissertation answers in the affirmative, privacy can be cultivated in these
centralized systems by utilizing cryptographic protocols.

I present here multiple avenues of research providing mechanisms for centralized
systems to offer services with privacy. These services are built upon
cryptographic protocols which limit information shared between users and the
services and have the smallest possible surface area allowing these centralized
systems to offer their services while fully enabling user privacy.

In order to develop these tools which guarantee user privacy there must be a
basis of what information is necessary for a service to function as well as how
potential bad acting Centralized Systems abuse the service they offer to
restrict access to the content they offer.

The concept of user privacy is inherently linked to the concept of service
provider censorship. A service with no user privacy can censor a users access to
content through that lack of privacy. By guaranteeing user privacy we restrict
the possibility of a service censoring content to users. 

This dissertation showcases three topics of enhancing user privacy in
centralized systems examining three different avenues of censorship. First we
examine content provider censorship, where a service that provides access to
content censors that access. I present a solution to this problem applicable for
content providers that wish to demonstrate their commitment to user privacy.
Second I examine metadata based privacy, where a service provider might censor
access to content not based on the content itself but based on the metadata
around that content such as who a message is sent from. I submit a cryptographic
method to achieve privacy around the metadata associated with end-to-end
encrypted messages, strengthening Sealed Sender, a technology geared towards
privacy that is vulnerable to a simple Statistical Disclosure Attack. Finally I
showcase a measurement study of DNS censorship over IPv6 to learn how countries
censor access to content differently over IPv6 compared to IPv4 motivating
avenues of censorship circumvention.

\section{Content Provider Censorship}
In 2016, Twitter received over 11,000 removal requests from government and
police agencies worldwide, and complied with 33\% of
them~\cite{twitter-transparency}. In one example from August 2016, Twitter
complied with a Brazilian government request to remove a tweet comparing
then-mayoral candidate Rafael Greca to a fictional villain from the 1992 film
Batman Returns~\cite{twitter-rafael-greca}. Between May 6 and May 19, 2021
Facebook removed nearly 500 pro-Palestine posts without any public
explanation~\cite{fb-palestine}.
% https://www.lumendatabase.org/notices/12866354

Services can be compelled (politically or financially) to censor content for
some subset of users which makes the censorship less overt: the content is
available for some users but not others. Can centralized systems be designed to
ensure they are transparent and auditable in terms of the content they remove?

\Cref{ch:poc}: \emph{Proof of Censorship: Enabling centralized
censorship-resistant content providers} answers this question affirmatively.
Content providers can be inherently auditable in order to be held accountable to
the content they remove access to.

Using Private Information Retrieval (PIR) a content provider is unaware of the
content a user is requesting thereby inhibiting its ability to censor access to
a subset of content. The provider must censor access to the service as a whole
or not at all.

\emph{Proof of Censorship} extends this functionality by guaranteeing the
content has not been modified by providing a cryptographic proof the content has
not been modified since upload time.

This further enables services to purposely remove content against its policies,
users receive proof that the content has been removed (or modified) that the
service can publicly announce.

\section{Metadata Privacy}
In addition to the content that users provide to services they also inherently
provide metadata associated with their content. Metadata is generally only
loosely based on the content it is related to. In the context of secure
end-to-end-encrypted messengers, such as Signal, the content of messages is
completely hidden from Signal and any other man in the middle. However, there is
metadata associated with encrypted messages, e.g., the sender and receiver of
the message, the time the message was sent, and in a more granular fashion, the
size of the message. Even centralized services that aim to protect content using
end-to-end encryption may receive sensitive metadata. For example, encrypted
messaging apps' servers might learn who is communicating, even if the content is
protected.

While leaking metadata may appear reasonable when compared to revealing the
contents of the messages, observing metadata can have serious consequences.
Consider that Alice may be a whistleblower communicating with a journalist
\cite{190976} or a survivor of domestic abuse seeking confidential support
\cite{236244}. In these cases, merely knowing \emph{to whom} Alice is
communicating combined with other contextual information is often enough to
infer conversation content without reading the messages themselves. Former NSA
and CIA director Michael Hayden succinctly illustrated this importance of
metadata when he said the US government ``kill[s] people based on metadata''
\cite{haydenmetadata}.

In 2018 Signal, a secure end-to-end encrypted messenger, introduced Sealed
Sender\cite{sealed-sender}, a protocol that provides a step in the correct
direction towards hiding metadata. 

In \Cref{ch:signal}: \emph{Improving Signal's Sealed Sender} I demonstrate that
the Sealed Sender protocol is vulnerable to a Statistical Disclosure Attack
(SDA), which would allow a malicious or compelled Signal (or other secure
end-to-end encrypted messengers that implement Sealed Sender) to link these
One-Way Anonymous messages due in part to delivery receipts: a feature of most
online messengers which immediately alert a user when their message has been
delivered.

Further, I develop a more private protocol by formalizing Sealed Sender
\emph{conversations} and decoupling a user's long term identity (such as Alice)
from the origin of messages, thereby creating a cryptographically private
One-Way Anonymous message. This framework is extended to Two-Way Anonymous
messages where the messaging service need not know either the sender nor the
receiver of a message. 

\section{Measurement of censorship in existing systems}
Internet censorship is a global problem that affects over half the world's
population. Censors rely on sophisticated network middleboxes to inspect and
block traffic. A core component of Internet censorship is DNS blocking, and
prior work has extensively studied how DNS censorship occurs, both for specific
countries~\cite{Anonymous2020:TripletCensors,USESEC21:GFWatch} and
globally~\cite{kuhrer2015going,dagon2008corrupted,pearce2017global,scott2016satellite}.

Nation-state level censorship systems function as centralized systems. Similar
to global services that have many servers located across the world, a
nation-state censorship system may have many middleboxes distributed across
their region to perform censorship, but the policies they follow on the whole
are centralized and limit the autonomy of the users in those countries.
\Cref{ch:v4vsv6}: \emph{Mind the IP Gap: Measuring the impact of IPv6 on DNS
censorship} works to study centralized censorship systems globally. 

IPv6 is becoming more widely deployed, with nearly 35\% of current Internet
traffic being served over native IPv6 connections~\cite{Google-IPv6}. It may
appear that censors fall into a bifurcation of either censoring over IPv6 or
not, but in practice we see that is not the case, we find a large range of how
well censors block DNS requests over IPv6 compared to IPv4.

By studying censorship in IPv6 we create opportunities for censorship tool
development to exploit the censorship techniques used by individual countries,
allowing citizens access to content they would not otherwise be able to reach.

In \Cref{ch:v4vsv6}: \emph{Mind the IP Gap: Measuring the impact of IPv6 on DNS
censorship} I present the results of our measurement of IPv6 DNS censorship
compared to IPv4. We find that censors show a clear bias towards censoring in
IPv4, and towards censoring native records (\texttt{A} records in IPv4 and
\texttt{AAAA} records in IPv6). We break down differences at the country level
statistically significant differences we discover by resolver and domain. We
find that Thailand, Myanmar, Bangladesh, Pakistan, and Iran all present
significant and consistent discrepancies across all resolvers and domains
suggesting avenues for successful censorship circumvention opportunities in
those countries.

\section{Thesis Structure}
In this thesis I provide three projects geared towards enhancing privacy in
centralized systems:

\begin{enumerate}
    \item Proof of Censorship --- this work showcases content provider
    censorship and how a service dedicated to being transparent about content it
    modifies or removes can implement a service which provides cryptographic
    promises that content has not been censored.
    \item Improving Signal's Sealed Sender --- Signal's Sealed Sender provided a
    first step towards user privacy but ultimately is vulnerable to a simple
    Statistical Disclosure Attack allowing Signal (if compelled or malicious) to
    determine who a user is messaging. This work provides a cryptographically
    private mechanism to ensure any centralized messaging service can offer its
    service without learning metadata information about which users are
    messaging each other.
    \item Mind the IP Gap --- this work measures how countries' centralized
    censorship systems block access to the internet through DNS resolution over
    the IPv4 and IPv6 internet allowing future work to implement systems allow
    censored citizens to circumvent countries DNS censorship techniques.
\end{enumerate}